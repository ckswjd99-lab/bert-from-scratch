{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mastering BERT Model: Building it from Scratch with Pytorch\n",
    "\n",
    "Creating and Exploring a BERT model from its most basic form, which is building it from the ground using pytorch\n",
    "\n",
    "Source: https://medium.com/data-and-beyond/complete-guide-to-building-bert-model-from-sratch-3e6562228891"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation\n",
    "\n",
    "- Download data\n",
    "- Import packages\n",
    "- Prepare datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers datasets tokenizers\n",
    "!(if [ ! -d \"./datasets\" ]; then \\\n",
    "    echo \"Downloading datasets\"; \\\n",
    "    wget http://www.cs.cornell.edu/~cristian/data/cornell_movie_dialogs_corpus.zip; \\\n",
    "    unzip -qq cornell_movie_dialogs_corpus.zip; \\\n",
    "    rm cornell_movie_dialogs_corpus.zip; \\\n",
    "    mkdir datasets; \\\n",
    "    mv cornell\\ movie-dialogs\\ corpus/movie_conversations.txt ./datasets; \\\n",
    "    mv cornell\\ movie-dialogs\\ corpus/movie_lines.txt ./datasets; \\\n",
    "    rm -rf cornell\\ movie-dialogs\\ corpus; \\\n",
    "fi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import random\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "from transformers import BertTokenizer\n",
    "import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from bert_dataset import BERTDataset\n",
    "from bert_model import BERT, BERTLM\n",
    "from bert_trainer import BERTTrainer\n",
    "\n",
    "import torch\n",
    "\n",
    "### setting seed\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "\n",
    "### setting device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "\n",
    "MAX_LEN = 64\n",
    "\n",
    "### loading all data into memory\n",
    "corpus_movie_conv = './datasets/movie_conversations.txt'\n",
    "corpus_movie_lines = './datasets/movie_lines.txt'\n",
    "with open(corpus_movie_conv, 'r', encoding='iso-8859-1') as c:\n",
    "    conv = c.readlines()\n",
    "with open(corpus_movie_lines, 'r', encoding='iso-8859-1') as l:\n",
    "    lines = l.readlines()\n",
    "\n",
    "### splitting text using special lines\n",
    "lines_dic = {}\n",
    "for line in lines:\n",
    "    objects = line.split(\" +++$+++ \")\n",
    "    lines_dic[objects[0]] = objects[-1]\n",
    "\n",
    "### generate question answer pairs\n",
    "pairs = []\n",
    "for con in conv:\n",
    "    ids = eval(con.split(\" +++$+++ \")[-1])\n",
    "    for i in range(len(ids)):\n",
    "        qa_pairs = []\n",
    "        \n",
    "        if i == len(ids) - 1:\n",
    "            break\n",
    "\n",
    "        first = lines_dic[ids[i]].strip()  \n",
    "        second = lines_dic[ids[i+1]].strip() \n",
    "\n",
    "        qa_pairs.append(' '.join(first.split()[:MAX_LEN]))\n",
    "        qa_pairs.append(' '.join(second.split()[:MAX_LEN]))\n",
    "        pairs.append(qa_pairs)\n",
    "\n",
    "print(pairs[20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare modules\n",
    "\n",
    "- Make WordPiece tokenizer\n",
    "- Prepare a dataset loader\n",
    "- Build a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WordPiece tokenizer\n",
    "\n",
    "### save data as txt file\n",
    "if not os.path.exists('./data'):\n",
    "    os.mkdir('./data')\n",
    "    text_data = []\n",
    "    file_count = 0\n",
    "\n",
    "    for sample in tqdm.tqdm([x[0] for x in pairs]):\n",
    "        text_data.append(sample)\n",
    "\n",
    "        # once we hit the 10K mark, save to file\n",
    "        if len(text_data) == 10000:\n",
    "            with open(f'./data/text_{file_count}.txt', 'w', encoding='utf-8') as fp:\n",
    "                fp.write('\\n'.join(text_data))\n",
    "            text_data = []\n",
    "            file_count += 1\n",
    "\n",
    "paths = [str(x) for x in Path('./data').glob('**/*.txt')]\n",
    "\n",
    "### training own tokenizer\n",
    "tokenizer = BertWordPieceTokenizer(\n",
    "    clean_text=True,\n",
    "    handle_chinese_chars=False,\n",
    "    strip_accents=False,\n",
    "    lowercase=True\n",
    ")\n",
    "\n",
    "tokenizer.train( \n",
    "    files=paths,\n",
    "    vocab_size=30_000, \n",
    "    min_frequency=5,\n",
    "    limit_alphabet=1000, \n",
    "    wordpieces_prefix='##',\n",
    "    special_tokens=['[PAD]', '[CLS]', '[SEP]', '[MASK]', '[UNK]']\n",
    "    )\n",
    "\n",
    "if not os.path.exists('./bert-it-1'):\n",
    "    os.mkdir('./bert-it-1')\n",
    "    tokenizer.save_model('./bert-it-1', 'bert-it')\n",
    "tokenizer = BertTokenizer.from_pretrained('./bert-it-1/bert-it-vocab.txt', local_files_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = BERTDataset(\n",
    "   pairs, seq_len=MAX_LEN, tokenizer=tokenizer)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "   train_data, batch_size=32, shuffle=True, pin_memory=True)\n",
    "\n",
    "print(train_data[random.randrange(len(train_data))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model = BERT(\n",
    "  vocab_size=len(tokenizer.vocab),\n",
    "  d_model=768,\n",
    "  n_layers=12,\n",
    "  heads=12,\n",
    "  dropout=0.1,\n",
    "  device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train BERT from scratch\n",
    "\n",
    "- Run training iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_lm = BERTLM(bert_model, len(tokenizer.vocab))\n",
    "bert_trainer = BERTTrainer(bert_lm, train_loader, device=device)\n",
    "epochs = 20\n",
    "\n",
    "for epoch in range(epochs):\n",
    "  bert_trainer.train(epoch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
